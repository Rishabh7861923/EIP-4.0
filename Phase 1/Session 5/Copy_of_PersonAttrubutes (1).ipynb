{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_0C1k5RJLYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KfZIfxE1ODB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "path = '/content/gdrive/My Drive/bestweights.h5'\n",
        "cp = ModelCheckpoint(path, verbose=1, monitor='val_loss', save_best_only=True, save_weights_only=False, mode='min', period=1)\n",
        "#callback = [cp]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation=augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32,augmentation=ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCBqK4jxaLeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Triangle cyclic learning rate policy\n",
        "\n",
        "from keras.callbacks import *\n",
        "\n",
        "class CyclicLR(Callback):\n",
        "    \n",
        "\n",
        "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "from keras.callbacks import *\n",
        "csv_logger = keras.callbacks.CSVLogger('training.log', separator=',', append=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0rtju6dRqEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import six\n",
        "from keras.models import Model\n",
        "from keras.layers import (\n",
        "    Input,\n",
        "    Activation,\n",
        "    Dense,\n",
        "    Flatten\n",
        ")\n",
        "from keras.layers.convolutional import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    AveragePooling2D\n",
        ")\n",
        "from keras.layers.merge import add\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def _bn_relu(input):\n",
        "    \"\"\"Helper to build a BN -> relu block\n",
        "    \"\"\"\n",
        "    norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n",
        "    return Activation(\"relu\")(norm)\n",
        "\n",
        "\n",
        "def _conv_bn_relu(**conv_params):\n",
        "    \"\"\"Helper to build a conv -> BN -> relu block\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        conv = Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(input)\n",
        "        return _bn_relu(conv)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _bn_relu_conv(**conv_params):\n",
        "    \"\"\"Helper to build a BN -> relu -> conv block.\n",
        "    This is an improved scheme proposed in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    filters = conv_params[\"filters\"]\n",
        "    kernel_size = conv_params[\"kernel_size\"]\n",
        "    strides = conv_params.setdefault(\"strides\", (1, 1))\n",
        "    kernel_initializer = conv_params.setdefault(\"kernel_initializer\", \"he_normal\")\n",
        "    padding = conv_params.setdefault(\"padding\", \"same\")\n",
        "    kernel_regularizer = conv_params.setdefault(\"kernel_regularizer\", l2(1.e-4))\n",
        "\n",
        "    def f(input):\n",
        "        activation = _bn_relu(input)\n",
        "        return Conv2D(filters=filters, kernel_size=kernel_size,\n",
        "                      strides=strides, padding=padding,\n",
        "                      kernel_initializer=kernel_initializer,\n",
        "                      kernel_regularizer=kernel_regularizer)(activation)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _shortcut(input, residual):\n",
        "    \"\"\"Adds a shortcut between input and residual block and merges them with \"sum\"\n",
        "    \"\"\"\n",
        "    # Expand channels of shortcut to match residual.\n",
        "    # Stride appropriately to match residual (width, height)\n",
        "    # Should be int if network architecture is correctly configured.\n",
        "    input_shape = K.int_shape(input)\n",
        "    residual_shape = K.int_shape(residual)\n",
        "    stride_width = int(round(input_shape[ROW_AXIS] / residual_shape[ROW_AXIS]))\n",
        "    stride_height = int(round(input_shape[COL_AXIS] / residual_shape[COL_AXIS]))\n",
        "    equal_channels = input_shape[CHANNEL_AXIS] == residual_shape[CHANNEL_AXIS]\n",
        "\n",
        "    shortcut = input\n",
        "    # 1 X 1 conv if shape is different. Else identity.\n",
        "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
        "        shortcut = Conv2D(filters=residual_shape[CHANNEL_AXIS],\n",
        "                          kernel_size=(1, 1),\n",
        "                          strides=(stride_width, stride_height),\n",
        "                          padding=\"valid\",\n",
        "                          kernel_initializer=\"he_normal\",\n",
        "                          kernel_regularizer=l2(0.0001))(input)\n",
        "\n",
        "    return add([shortcut, residual])\n",
        "\n",
        "\n",
        "def _residual_block(block_function, filters, repetitions, is_first_layer=False):\n",
        "    \"\"\"Builds a residual block with repeating bottleneck blocks.\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "        for i in range(repetitions):\n",
        "            init_strides = (1, 1)\n",
        "            if i == 0 and not is_first_layer:\n",
        "                init_strides = (2, 2)\n",
        "            input = block_function(filters=filters, init_strides=init_strides,\n",
        "                                   is_first_block_of_first_layer=(is_first_layer and i == 0))(input)\n",
        "        return input\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def basic_block(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Basic 3 X 3 convolution blocks for use on resnets with layers <= 34.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv1 = Conv2D(filters=filters, kernel_size=(3, 3),\n",
        "                           strides=init_strides,\n",
        "                           padding=\"same\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv1 = _bn_relu_conv(filters=filters, kernel_size=(3, 3),\n",
        "                                  strides=init_strides)(input)\n",
        "\n",
        "        residual = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv1)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def bottleneck(filters, init_strides=(1, 1), is_first_block_of_first_layer=False):\n",
        "    \"\"\"Bottleneck architecture for > 34 layer resnet.\n",
        "    Follows improved proposed scheme in http://arxiv.org/pdf/1603.05027v2.pdf\n",
        "    Returns:\n",
        "        A final conv layer of filters * 4\n",
        "    \"\"\"\n",
        "    def f(input):\n",
        "\n",
        "        if is_first_block_of_first_layer:\n",
        "            # don't repeat bn->relu since we just did bn->relu->maxpool\n",
        "            conv_1_1 = Conv2D(filters=filters, kernel_size=(1, 1),\n",
        "                              strides=init_strides,\n",
        "                              padding=\"same\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=l2(1e-4))(input)\n",
        "        else:\n",
        "            conv_1_1 = _bn_relu_conv(filters=filters, kernel_size=(1, 1),\n",
        "                                     strides=init_strides)(input)\n",
        "\n",
        "        conv_3_3 = _bn_relu_conv(filters=filters, kernel_size=(3, 3))(conv_1_1)\n",
        "        residual = _bn_relu_conv(filters=filters * 4, kernel_size=(1, 1))(conv_3_3)\n",
        "        return _shortcut(input, residual)\n",
        "\n",
        "    return f\n",
        "\n",
        "\n",
        "def _handle_dim_ordering():\n",
        "    global ROW_AXIS\n",
        "    global COL_AXIS\n",
        "    global CHANNEL_AXIS\n",
        "    if K.common.image_dim_ordering() == 'tf':\n",
        "        ROW_AXIS = 1\n",
        "        COL_AXIS = 2\n",
        "        CHANNEL_AXIS = 3\n",
        "    else:\n",
        "        CHANNEL_AXIS = 1\n",
        "        ROW_AXIS = 2\n",
        "        COL_AXIS = 3\n",
        "\n",
        "\n",
        "def _get_block(identifier):\n",
        "    if isinstance(identifier, six.string_types):\n",
        "        res = globals().get(identifier)\n",
        "        if not res:\n",
        "            raise ValueError('Invalid {}'.format(identifier))\n",
        "        return res\n",
        "    return identifier\n",
        "\n",
        "\n",
        "class ResnetBuilder(object):\n",
        "    @staticmethod\n",
        "    def build(input_shape, num_outputs, block_fn, repetitions):\n",
        "        \"\"\"Builds a custom ResNet like architecture.\n",
        "        Args:\n",
        "            input_shape: The input shape in the form (nb_channels, nb_rows, nb_cols)\n",
        "            num_outputs: The number of outputs at final softmax layer\n",
        "            block_fn: The block function to use. This is either `basic_block` or `bottleneck`.\n",
        "                The original paper used basic_block for layers < 50\n",
        "            repetitions: Number of repetitions of various block units.\n",
        "                At each block unit, the number of filters are doubled and the input size is halved\n",
        "        Returns:\n",
        "            The keras `Model`.\n",
        "        \"\"\"\n",
        "        _handle_dim_ordering()\n",
        "        if len(input_shape) != 3:\n",
        "            raise Exception(\"Input shape should be a tuple (nb_channels, nb_rows, nb_cols)\")\n",
        "\n",
        "        # Permute dimension order if necessary\n",
        "        if K.common.image_dim_ordering() == 'tf':\n",
        "            input_shape = (input_shape[1], input_shape[2], input_shape[0])\n",
        "\n",
        "        # Load function from str if needed.\n",
        "        block_fn = _get_block(block_fn)\n",
        "\n",
        "        input = Input(shape=input_shape)\n",
        "        conv1 = _conv_bn_relu(filters=64, kernel_size=(7, 7), strides=(2, 2))(input)\n",
        "        pool1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\")(conv1)\n",
        "\n",
        "        block = pool1\n",
        "        filters = 64\n",
        "        for i, r in enumerate(repetitions):\n",
        "            block = _residual_block(block_fn, filters=filters, repetitions=r, is_first_layer=(i == 0))(block)\n",
        "            filters *= 2\n",
        "\n",
        "        # Last activation\n",
        "        block = _bn_relu(block)\n",
        "\n",
        "        # Classifier block\n",
        "        block_shape = K.int_shape(block)\n",
        "        pool2 = AveragePooling2D(pool_size=(block_shape[ROW_AXIS], block_shape[COL_AXIS]),\n",
        "                                 strides=(1, 1))(block)\n",
        "        flatten1 = Flatten()(pool2)\n",
        "        dense = Dense(units=num_outputs, kernel_initializer=\"he_normal\",\n",
        "                      activation=\"softmax\")(flatten1)\n",
        "\n",
        "        model = Model(inputs=input, outputs=dense)\n",
        "        return model\n",
        "    @staticmethod\n",
        "    def build_resnet_18(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [2, 2, 2, 2])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_34(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, basic_block, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_50(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_101(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 4, 23, 3])\n",
        "\n",
        "    @staticmethod\n",
        "    def build_resnet_152(input_shape, num_outputs):\n",
        "        return ResnetBuilder.build(input_shape, num_outputs, bottleneck, [3, 8, 36, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backbone = ResnetBuilder.build_resnet_18((3,224,224), 8)\n",
        "\n",
        "neck = backbone.output\n",
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = {\"gender_output\": \"binary_crossentropy\",\"image_quality_output\": \"categorical_crossentropy\",\n",
        "\"age_output\": \"categorical_crossentropy\",\n",
        "\"weight_output\": \"categorical_crossentropy\",\n",
        "}\n",
        "loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "clr = CyclicLR(base_lr=0.001, max_lr=0.1,\n",
        "                        step_size=782.)\n",
        "from keras import optimizers\n",
        "opt = keras.optimizers.SGD(lr=0.001,momentum=1e-2)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing = True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[clr, csv_logger]\n",
        ")\n",
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwS0AF1Yfoo7",
        "colab_type": "code",
        "outputId": "5595ccf0-9e5e-4ff9-f3f3-77790ca6e86c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing = True,\n",
        "    workers=6, \n",
        "    epochs=50,\n",
        "    verbose=1,\n",
        "    callbacks=[clr, csv_logger]\n",
        ")\n",
        "results = model.evaluate_generator(valid_gen, verbose=1)\n",
        "dict(zip(model.metrics_names, results))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1168 - gender_output_loss: 0.1141 - image_quality_output_loss: 0.4446 - age_output_loss: 1.3366 - weight_output_loss: 0.9342 - bag_output_loss: 0.8204 - footwear_output_loss: 0.4792 - pose_output_loss: 0.3649 - emotion_output_loss: 0.8367 - gender_output_acc: 0.9570 - image_quality_output_acc: 0.8021 - age_output_acc: 0.4348 - weight_output_acc: 0.6622 - bag_output_acc: 0.6287 - footwear_output_acc: 0.7861 - pose_output_acc: 0.8531 - emotion_output_acc: 0.7120\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 6.1169 - gender_output_loss: 0.1142 - image_quality_output_loss: 0.4445 - age_output_loss: 1.3367 - weight_output_loss: 0.9342 - bag_output_loss: 0.8204 - footwear_output_loss: 0.4795 - pose_output_loss: 0.3649 - emotion_output_loss: 0.8364 - gender_output_acc: 0.9570 - image_quality_output_acc: 0.8021 - age_output_acc: 0.4348 - weight_output_acc: 0.6623 - bag_output_acc: 0.6287 - footwear_output_acc: 0.7860 - pose_output_acc: 0.8530 - emotion_output_acc: 0.7122 - val_loss: 9.0477 - val_gender_output_loss: 0.6670 - val_image_quality_output_loss: 1.5094 - val_age_output_loss: 1.4041 - val_weight_output_loss: 0.9616 - val_bag_output_loss: 0.8742 - val_footwear_output_loss: 1.0315 - val_pose_output_loss: 0.9247 - val_emotion_output_loss: 0.8902 - val_gender_output_acc: 0.8059 - val_image_quality_output_acc: 0.5605 - val_age_output_acc: 0.4088 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6694 - val_pose_output_acc: 0.6472 - val_emotion_output_acc: 0.7087\n",
            "\n",
            "Epoch 2/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.9208 - gender_output_loss: 0.0949 - image_quality_output_loss: 0.3646 - age_output_loss: 1.3273 - weight_output_loss: 0.9271 - bag_output_loss: 0.8145 - footwear_output_loss: 0.4482 - pose_output_loss: 0.3229 - emotion_output_loss: 0.8367 - gender_output_acc: 0.9625 - image_quality_output_acc: 0.8446 - age_output_acc: 0.4364 - weight_output_acc: 0.6634 - bag_output_acc: 0.6330 - footwear_output_acc: 0.7998 - pose_output_acc: 0.8753 - emotion_output_acc: 0.7117 - val_loss: 10.1617 - val_gender_output_loss: 0.7560 - val_image_quality_output_loss: 1.5829 - val_age_output_loss: 1.4138 - val_weight_output_loss: 0.9656 - val_bag_output_loss: 0.8805 - val_footwear_output_loss: 1.0718 - val_pose_output_loss: 1.7839 - val_emotion_output_loss: 0.9238 - val_gender_output_acc: 0.7913 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6613 - val_pose_output_acc: 0.4793 - val_emotion_output_acc: 0.7087\n",
            "Epoch 3/50\n",
            "\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 6.1855 - gender_output_loss: 0.1159 - image_quality_output_loss: 0.4752 - age_output_loss: 1.3382 - weight_output_loss: 0.9327 - bag_output_loss: 0.8216 - footwear_output_loss: 0.4914 - pose_output_loss: 0.3880 - emotion_output_loss: 0.8401 - gender_output_acc: 0.9554 - image_quality_output_acc: 0.7812 - age_output_acc: 0.4304 - weight_output_acc: 0.6620 - bag_output_acc: 0.6314 - footwear_output_acc: 0.7850 - pose_output_acc: 0.8460 - emotion_output_acc: 0.7117 - val_loss: 9.5658 - val_gender_output_loss: 0.7859 - val_image_quality_output_loss: 1.2543 - val_age_output_loss: 1.4108 - val_weight_output_loss: 0.9678 - val_bag_output_loss: 0.8821 - val_footwear_output_loss: 1.1775 - val_pose_output_loss: 1.4038 - val_emotion_output_loss: 0.8999 - val_gender_output_acc: 0.7540 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.5771 - val_footwear_output_acc: 0.6467 - val_pose_output_acc: 0.4834 - val_emotion_output_acc: 0.7087\n",
            "Epoch 4/50\n",
            "360/360 [==============================] - 45s 126ms/step - loss: 6.5767 - gender_output_loss: 0.1839 - image_quality_output_loss: 0.5980 - age_output_loss: 1.3569 - weight_output_loss: 0.9463 - bag_output_loss: 0.8296 - footwear_output_loss: 0.5492 - pose_output_loss: 0.4792 - emotion_output_loss: 0.8464 - gender_output_acc: 0.9294 - image_quality_output_acc: 0.7191 - age_output_acc: 0.4241 - weight_output_acc: 0.6517 - bag_output_acc: 0.6265 - footwear_output_acc: 0.7582 - pose_output_acc: 0.8048 - emotion_output_acc: 0.7118 - val_loss: 9.3068 - val_gender_output_loss: 0.8593 - val_image_quality_output_loss: 1.3408 - val_age_output_loss: 1.5061 - val_weight_output_loss: 0.9962 - val_bag_output_loss: 0.9194 - val_footwear_output_loss: 1.1441 - val_pose_output_loss: 0.8470 - val_emotion_output_loss: 0.9049 - val_gender_output_acc: 0.7858 - val_image_quality_output_acc: 0.5363 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6084 - val_bag_output_acc: 0.5847 - val_footwear_output_acc: 0.6351 - val_pose_output_acc: 0.6850 - val_emotion_output_acc: 0.7087\n",
            "Epoch 5/50\n",
            "360/360 [==============================] - 45s 126ms/step - loss: 6.1698 - gender_output_loss: 0.1229 - image_quality_output_loss: 0.4744 - age_output_loss: 1.3372 - weight_output_loss: 0.9327 - bag_output_loss: 0.8229 - footwear_output_loss: 0.4925 - pose_output_loss: 0.3606 - emotion_output_loss: 0.8390 - gender_output_acc: 0.9518 - image_quality_output_acc: 0.7857 - age_output_acc: 0.4332 - weight_output_acc: 0.6612 - bag_output_acc: 0.6324 - footwear_output_acc: 0.7834 - pose_output_acc: 0.8556 - emotion_output_acc: 0.7121 - val_loss: 9.4994 - val_gender_output_loss: 0.7437 - val_image_quality_output_loss: 1.8863 - val_age_output_loss: 1.4138 - val_weight_output_loss: 0.9606 - val_bag_output_loss: 0.8779 - val_footwear_output_loss: 1.1049 - val_pose_output_loss: 0.8300 - val_emotion_output_loss: 0.8956 - val_gender_output_acc: 0.7878 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4007 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6568 - val_pose_output_acc: 0.6769 - val_emotion_output_acc: 0.7087\n",
            "Epoch 6/50\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 5/50\n",
            "360/360 [==============================] - 45s 126ms/step - loss: 5.8742 - gender_output_loss: 0.0927 - image_quality_output_loss: 0.3478 - age_output_loss: 1.3231 - weight_output_loss: 0.9232 - bag_output_loss: 0.8135 - footwear_output_loss: 0.4560 - pose_output_loss: 0.2989 - emotion_output_loss: 0.8331 - gender_output_acc: 0.9663 - image_quality_output_acc: 0.8480 - age_output_acc: 0.4405 - weight_output_acc: 0.6669 - bag_output_acc: 0.6343 - footwear_output_acc: 0.7947 - pose_output_acc: 0.8833 - emotion_output_acc: 0.7123 - val_loss: 9.6555 - val_gender_output_loss: 0.6725 - val_image_quality_output_loss: 1.7826 - val_age_output_loss: 1.4063 - val_weight_output_loss: 0.9600 - val_bag_output_loss: 0.8750 - val_footwear_output_loss: 1.0709 - val_pose_output_loss: 1.2060 - val_emotion_output_loss: 0.8970 - val_gender_output_acc: 0.8125 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.4057 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.6598 - val_pose_output_acc: 0.5559 - val_emotion_output_acc: 0.7087\n",
            "Epoch 7/50\n",
            "Epoch 6/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.9194 - gender_output_loss: 0.0974 - image_quality_output_loss: 0.3761 - age_output_loss: 1.3221 - weight_output_loss: 0.9212 - bag_output_loss: 0.8163 - footwear_output_loss: 0.4531 - pose_output_loss: 0.3121 - emotion_output_loss: 0.8373 - gender_output_acc: 0.9619 - image_quality_output_acc: 0.8306 - age_output_acc: 0.4405 - weight_output_acc: 0.6670 - bag_output_acc: 0.6319 - footwear_output_acc: 0.7956 - pose_output_acc: 0.8745 - emotion_output_acc: 0.7123 - val_loss: 9.9937 - val_gender_output_loss: 0.9724 - val_image_quality_output_loss: 1.6436 - val_age_output_loss: 1.4599 - val_weight_output_loss: 0.9804 - val_bag_output_loss: 0.9144 - val_footwear_output_loss: 1.4129 - val_pose_output_loss: 0.9300 - val_emotion_output_loss: 0.8969 - val_gender_output_acc: 0.7878 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.3795 - val_weight_output_acc: 0.6270 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6492 - val_pose_output_acc: 0.6598 - val_emotion_output_acc: 0.7082\n",
            "Epoch 8/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 6.4248 - gender_output_loss: 0.1527 - image_quality_output_loss: 0.5652 - age_output_loss: 1.3469 - weight_output_loss: 0.9381 - bag_output_loss: 0.8292 - footwear_output_loss: 0.5273 - pose_output_loss: 0.4331 - emotion_output_loss: 0.8464 - gender_output_acc: 0.9413 - image_quality_output_acc: 0.7381 - age_output_acc: 0.4245 - weight_output_acc: 0.6559 - bag_output_acc: 0.6296 - footwear_output_acc: 0.7696 - pose_output_acc: 0.8246 - emotion_output_acc: 0.7119 - val_loss: 10.6624 - val_gender_output_loss: 0.7915 - val_image_quality_output_loss: 1.4001 - val_age_output_loss: 1.4326 - val_weight_output_loss: 0.9709 - val_bag_output_loss: 0.8995 - val_footwear_output_loss: 2.4224 - val_pose_output_loss: 1.0500 - val_emotion_output_loss: 0.9050 - val_gender_output_acc: 0.7787 - val_image_quality_output_acc: 0.5449 - val_age_output_acc: 0.3936 - val_weight_output_acc: 0.6351 - val_bag_output_acc: 0.5927 - val_footwear_output_acc: 0.4551 - val_pose_output_acc: 0.5721 - val_emotion_output_acc: 0.7087\n",
            "Epoch 9/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 6.2244 - gender_output_loss: 0.1378 - image_quality_output_loss: 0.4980 - age_output_loss: 1.3332 - weight_output_loss: 0.9339 - bag_output_loss: 0.8229 - footwear_output_loss: 0.4905 - pose_output_loss: 0.3770 - emotion_output_loss: 0.8414 - gender_output_acc: 0.9470 - image_quality_output_acc: 0.7716 - age_output_acc: 0.4347 - weight_output_acc: 0.6586 - bag_output_acc: 0.6296 - footwear_output_acc: 0.7805 - pose_output_acc: 0.8461 - emotion_output_acc: 0.7123 - val_loss: 9.0039 - val_gender_output_loss: 0.7021 - val_image_quality_output_loss: 1.3745 - val_age_output_loss: 1.4029 - val_weight_output_loss: 0.9625 - val_bag_output_loss: 0.8807 - val_footwear_output_loss: 1.0652 - val_pose_output_loss: 0.9388 - val_emotion_output_loss: 0.8884 - val_gender_output_acc: 0.7949 - val_image_quality_output_acc: 0.5519 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6618 - val_pose_output_acc: 0.6623 - val_emotion_output_acc: 0.7087\n",
            "Epoch 10/50\n",
            "360/360 [==============================] - 47s 130ms/step - loss: 5.8368 - gender_output_loss: 0.0954 - image_quality_output_loss: 0.3464 - age_output_loss: 1.3094 - weight_output_loss: 0.9147 - bag_output_loss: 0.8173 - footwear_output_loss: 0.4487 - pose_output_loss: 0.2812 - emotion_output_loss: 0.8362 - gender_output_acc: 0.9660 - image_quality_output_acc: 0.8449 - age_output_acc: 0.4467 - weight_output_acc: 0.6717 - bag_output_acc: 0.6323 - footwear_output_acc: 0.7998 - pose_output_acc: 0.8894 - emotion_output_acc: 0.7125 - val_loss: 9.4992 - val_gender_output_loss: 0.7115 - val_image_quality_output_loss: 1.6760 - val_age_output_loss: 1.4132 - val_weight_output_loss: 0.9608 - val_bag_output_loss: 0.8745 - val_footwear_output_loss: 1.1064 - val_pose_output_loss: 1.0805 - val_emotion_output_loss: 0.8893 - val_gender_output_acc: 0.8049 - val_image_quality_output_acc: 0.5670 - val_age_output_acc: 0.3972 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.6557 - val_pose_output_acc: 0.6144 - val_emotion_output_acc: 0.7087\n",
            "Epoch 11/50\n",
            "360/360 [==============================] - 47s 130ms/step - loss: 5.7606 - gender_output_loss: 0.0838 - image_quality_output_loss: 0.3230 - age_output_loss: 1.3052 - weight_output_loss: 0.9144 - bag_output_loss: 0.8144 - footwear_output_loss: 0.4449 - pose_output_loss: 0.2583 - emotion_output_loss: 0.8308 - gender_output_acc: 0.9688 - image_quality_output_acc: 0.8556 - age_output_acc: 0.4493 - weight_output_acc: 0.6698 - bag_output_acc: 0.6347 - footwear_output_acc: 0.8003 - pose_output_acc: 0.9000 - emotion_output_acc: 0.7128 - val_loss: 9.9731 - val_gender_output_loss: 0.6915 - val_image_quality_output_loss: 1.8638 - val_age_output_loss: 1.4112 - val_weight_output_loss: 0.9706 - val_bag_output_loss: 0.8878 - val_footwear_output_loss: 1.4848 - val_pose_output_loss: 0.9853 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.7954 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5786 - val_footwear_output_acc: 0.6356 - val_pose_output_acc: 0.6794 - val_emotion_output_acc: 0.7087\n",
            "Epoch 12/50\n",
            "360/360 [==============================] - 45s 126ms/step - loss: 6.1390 - gender_output_loss: 0.1280 - image_quality_output_loss: 0.4580 - age_output_loss: 1.3211 - weight_output_loss: 0.9234 - bag_output_loss: 0.8182 - footwear_output_loss: 0.4923 - pose_output_loss: 0.3711 - emotion_output_loss: 0.8420 - gender_output_acc: 0.9498 - image_quality_output_acc: 0.7931 - age_output_acc: 0.4390 - weight_output_acc: 0.6655 - bag_output_acc: 0.6329 - footwear_output_acc: 0.7828 - pose_output_acc: 0.8511 - emotion_output_acc: 0.7121 - val_loss: 9.9133 - val_gender_output_loss: 0.6694 - val_image_quality_output_loss: 2.3985 - val_age_output_loss: 1.4283 - val_weight_output_loss: 0.9752 - val_bag_output_loss: 0.8900 - val_footwear_output_loss: 1.0052 - val_pose_output_loss: 0.8562 - val_emotion_output_loss: 0.9021 - val_gender_output_acc: 0.7540 - val_image_quality_output_acc: 0.4677 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5791 - val_footwear_output_acc: 0.6255 - val_pose_output_acc: 0.6391 - val_emotion_output_acc: 0.7087\n",
            "Epoch 13/50\n",
            "360/360 [==============================] - 46s 126ms/step - loss: 6.3086 - gender_output_loss: 0.1564 - image_quality_output_loss: 0.5199 - age_output_loss: 1.3308 - weight_output_loss: 0.9299 - bag_output_loss: 0.8259 - footwear_output_loss: 0.5158 - pose_output_loss: 0.3952 - emotion_output_loss: 0.8437 - gender_output_acc: 0.9415 - image_quality_output_acc: 0.7668 - age_output_acc: 0.4368 - weight_output_acc: 0.6603 - bag_output_acc: 0.6317 - footwear_output_acc: 0.7747 - pose_output_acc: 0.8378 - emotion_output_acc: 0.7131 - val_loss: 9.2208 - val_gender_output_loss: 0.6745 - val_image_quality_output_loss: 1.5847 - val_age_output_loss: 1.4117 - val_weight_output_loss: 0.9674 - val_bag_output_loss: 0.8758 - val_footwear_output_loss: 1.0501 - val_pose_output_loss: 0.9781 - val_emotion_output_loss: 0.8874 - val_gender_output_acc: 0.7757 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5817 - val_footwear_output_acc: 0.6406 - val_pose_output_acc: 0.6361 - val_emotion_output_acc: 0.7087\n",
            "Epoch 14/50\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 5.8557 - gender_output_loss: 0.0943 - image_quality_output_loss: 0.3588 - age_output_loss: 1.3067 - weight_output_loss: 0.9123 - bag_output_loss: 0.8181 - footwear_output_loss: 0.4539 - pose_output_loss: 0.2877 - emotion_output_loss: 0.8340 - gender_output_acc: 0.9648 - image_quality_output_acc: 0.8399 - age_output_acc: 0.4461 - weight_output_acc: 0.6704 - bag_output_acc: 0.6350 - footwear_output_acc: 0.7956 - pose_output_acc: 0.8859 - emotion_output_acc: 0.7137 - val_loss: 9.7511 - val_gender_output_loss: 0.7231 - val_image_quality_output_loss: 1.8363 - val_age_output_loss: 1.4300 - val_weight_output_loss: 0.9605 - val_bag_output_loss: 0.8790 - val_footwear_output_loss: 1.1150 - val_pose_output_loss: 1.1260 - val_emotion_output_loss: 0.8925 - val_gender_output_acc: 0.7994 - val_image_quality_output_acc: 0.5650 - val_age_output_acc: 0.3851 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6598 - val_pose_output_acc: 0.5943 - val_emotion_output_acc: 0.7087\n",
            "Epoch 15/50\n",
            "\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.6612 - gender_output_loss: 0.0805 - image_quality_output_loss: 0.2846 - age_output_loss: 1.2929 - weight_output_loss: 0.9022 - bag_output_loss: 0.8123 - footwear_output_loss: 0.4399 - pose_output_loss: 0.2332 - emotion_output_loss: 0.8274 - gender_output_acc: 0.9694 - image_quality_output_acc: 0.8764 - age_output_acc: 0.4540 - weight_output_acc: 0.6773 - bag_output_acc: 0.6378 - footwear_output_acc: 0.8026 - pose_output_acc: 0.9113 - emotion_output_acc: 0.7145 - val_loss: 10.1622 - val_gender_output_loss: 0.8560 - val_image_quality_output_loss: 1.9889 - val_age_output_loss: 1.4083 - val_weight_output_loss: 0.9627 - val_bag_output_loss: 0.8756 - val_footwear_output_loss: 1.1232 - val_pose_output_loss: 1.2611 - val_emotion_output_loss: 0.8992 - val_gender_output_acc: 0.7671 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.5842 - val_footwear_output_acc: 0.6512 - val_pose_output_acc: 0.5953 - val_emotion_output_acc: 0.7077\n",
            "Epoch 16/50\n",
            "360/360 [==============================] - 47s 129ms/step - loss: 5.7877 - gender_output_loss: 0.0888 - image_quality_output_loss: 0.3378 - age_output_loss: 1.3012 - weight_output_loss: 0.9074 - bag_output_loss: 0.8150 - footwear_output_loss: 0.4526 - pose_output_loss: 0.2670 - emotion_output_loss: 0.8325 - gender_output_acc: 0.9639 - image_quality_output_acc: 0.8490 - age_output_acc: 0.4534 - weight_output_acc: 0.6737 - bag_output_acc: 0.6397 - footwear_output_acc: 0.8008 - pose_output_acc: 0.8939 - emotion_output_acc: 0.7148 - val_loss: 13.2730 - val_gender_output_loss: 1.0353 - val_image_quality_output_loss: 1.3808 - val_age_output_loss: 1.4650 - val_weight_output_loss: 0.9936 - val_bag_output_loss: 0.8894 - val_footwear_output_loss: 2.9143 - val_pose_output_loss: 2.8839 - val_emotion_output_loss: 0.9254 - val_gender_output_acc: 0.7399 - val_image_quality_output_acc: 0.5071 - val_age_output_acc: 0.3881 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5640 - val_footwear_output_acc: 0.5076 - val_pose_output_acc: 0.4178 - val_emotion_output_acc: 0.7072\n",
            "Epoch 17/50\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 6.3280 - gender_output_loss: 0.1597 - image_quality_output_loss: 0.5165 - age_output_loss: 1.3285 - weight_output_loss: 0.9263 - bag_output_loss: 0.8278 - footwear_output_loss: 0.5200 - pose_output_loss: 0.4145 - emotion_output_loss: 0.8447 - gender_output_acc: 0.9391 - image_quality_output_acc: 0.7642 - age_output_acc: 0.4360 - weight_output_acc: 0.6618 - bag_output_acc: 0.6301 - footwear_output_acc: 0.7693 - pose_output_acc: 0.8382 - emotion_output_acc: 0.7122 - val_loss: 9.9447 - val_gender_output_loss: 0.7543 - val_image_quality_output_loss: 1.9822 - val_age_output_loss: 1.4354 - val_weight_output_loss: 0.9757 - val_bag_output_loss: 0.8903 - val_footwear_output_loss: 1.1343 - val_pose_output_loss: 1.0704 - val_emotion_output_loss: 0.9089 - val_gender_output_acc: 0.7530 - val_image_quality_output_acc: 0.5237 - val_age_output_acc: 0.3836 - val_weight_output_acc: 0.6376 - val_bag_output_acc: 0.5862 - val_footwear_output_acc: 0.6517 - val_pose_output_acc: 0.6568 - val_emotion_output_acc: 0.7036\n",
            "Epoch 18/50\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 5.9247 - gender_output_loss: 0.1022 - image_quality_output_loss: 0.3789 - age_output_loss: 1.3071 - weight_output_loss: 0.9131 - bag_output_loss: 0.8198 - footwear_output_loss: 0.4775 - pose_output_loss: 0.3019 - emotion_output_loss: 0.8323 - gender_output_acc: 0.9626 - image_quality_output_acc: 0.8351 - age_output_acc: 0.4489 - weight_output_acc: 0.6705 - bag_output_acc: 0.6309 - footwear_output_acc: 0.7866 - pose_output_acc: 0.8842 - emotion_output_acc: 0.7142 - val_loss: 9.7333 - val_gender_output_loss: 0.6743 - val_image_quality_output_loss: 1.8689 - val_age_output_loss: 1.4139 - val_weight_output_loss: 0.9642 - val_bag_output_loss: 0.8823 - val_footwear_output_loss: 1.1940 - val_pose_output_loss: 1.0524 - val_emotion_output_loss: 0.8926 - val_gender_output_acc: 0.8039 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3997 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6542 - val_pose_output_acc: 0.6351 - val_emotion_output_acc: 0.7087\n",
            "Epoch 19/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.6431 - gender_output_loss: 0.0854 - image_quality_output_loss: 0.2809 - age_output_loss: 1.2824 - weight_output_loss: 0.8957 - bag_output_loss: 0.8143 - footwear_output_loss: 0.4397 - pose_output_loss: 0.2260 - emotion_output_loss: 0.8288 - gender_output_acc: 0.9681 - image_quality_output_acc: 0.8821 - age_output_acc: 0.4601 - weight_output_acc: 0.6789 - bag_output_acc: 0.6375 - footwear_output_acc: 0.8086 - pose_output_acc: 0.9165 - emotion_output_acc: 0.7163 - val_loss: 9.9377 - val_gender_output_loss: 0.6984 - val_image_quality_output_loss: 2.0926 - val_age_output_loss: 1.4229 - val_weight_output_loss: 0.9605 - val_bag_output_loss: 0.8719 - val_footwear_output_loss: 1.1503 - val_pose_output_loss: 1.0619 - val_emotion_output_loss: 0.8900 - val_gender_output_acc: 0.8085 - val_image_quality_output_acc: 0.5701 - val_age_output_acc: 0.3891 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.5927 - val_footwear_output_acc: 0.6608 - val_pose_output_acc: 0.6447 - val_emotion_output_acc: 0.7087\n",
            "Epoch 20/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.6297 - gender_output_loss: 0.0787 - image_quality_output_loss: 0.2842 - age_output_loss: 1.2774 - weight_output_loss: 0.8958 - bag_output_loss: 0.8177 - footwear_output_loss: 0.4472 - pose_output_loss: 0.2151 - emotion_output_loss: 0.8259 - gender_output_acc: 0.9691 - image_quality_output_acc: 0.8789 - age_output_acc: 0.4575 - weight_output_acc: 0.6791 - bag_output_acc: 0.6345 - footwear_output_acc: 0.8038 - pose_output_acc: 0.9193 - emotion_output_acc: 0.7163 - val_loss: 12.0777 - val_gender_output_loss: 1.0098 - val_image_quality_output_loss: 1.9499 - val_age_output_loss: 1.4845 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.8818 - val_footwear_output_loss: 2.4423 - val_pose_output_loss: 1.6332 - val_emotion_output_loss: 0.9071 - val_gender_output_acc: 0.7727 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.3770 - val_weight_output_acc: 0.6310 - val_bag_output_acc: 0.5827 - val_footwear_output_acc: 0.5504 - val_pose_output_acc: 0.4788 - val_emotion_output_acc: 0.7082\n",
            "Epoch 21/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 6.1271 - gender_output_loss: 0.1363 - image_quality_output_loss: 0.4798 - age_output_loss: 1.3072 - weight_output_loss: 0.9193 - bag_output_loss: 0.8250 - footwear_output_loss: 0.4813 - pose_output_loss: 0.3484 - emotion_output_loss: 0.8410 - gender_output_acc: 0.9465 - image_quality_output_acc: 0.7899 - age_output_acc: 0.4493 - weight_output_acc: 0.6673 - bag_output_acc: 0.6347 - footwear_output_acc: 0.7896 - pose_output_acc: 0.8632 - emotion_output_acc: 0.7141 - val_loss: 10.4264 - val_gender_output_loss: 0.8033 - val_image_quality_output_loss: 1.5705 - val_age_output_loss: 1.4137 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.8946 - val_footwear_output_loss: 1.4612 - val_pose_output_loss: 1.5798 - val_emotion_output_loss: 0.9349 - val_gender_output_acc: 0.7278 - val_image_quality_output_acc: 0.5469 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5650 - val_footwear_output_acc: 0.6074 - val_pose_output_acc: 0.4415 - val_emotion_output_acc: 0.7087\n",
            "Epoch 22/50\n",
            "360/360 [==============================] - 45s 126ms/step - loss: 5.9874 - gender_output_loss: 0.1229 - image_quality_output_loss: 0.4006 - age_output_loss: 1.3039 - weight_output_loss: 0.9142 - bag_output_loss: 0.8247 - footwear_output_loss: 0.4780 - pose_output_loss: 0.3148 - emotion_output_loss: 0.8348 - gender_output_acc: 0.9525 - image_quality_output_acc: 0.8230 - age_output_acc: 0.4490 - weight_output_acc: 0.6685 - bag_output_acc: 0.6342 - footwear_output_acc: 0.7880 - pose_output_acc: 0.8748 - emotion_output_acc: 0.7137 - val_loss: 10.4184 - val_gender_output_loss: 0.7650 - val_image_quality_output_loss: 1.9066 - val_age_output_loss: 1.4525 - val_weight_output_loss: 0.9589 - val_bag_output_loss: 0.8985 - val_footwear_output_loss: 1.0874 - val_pose_output_loss: 1.6501 - val_emotion_output_loss: 0.9070 - val_gender_output_acc: 0.7923 - val_image_quality_output_acc: 0.5585 - val_age_output_acc: 0.3750 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.5822 - val_footwear_output_acc: 0.6517 - val_pose_output_acc: 0.4990 - val_emotion_output_acc: 0.7087\n",
            "Epoch 23/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 6.1271 - gender_output_loss: 0.1363 - image_quality_output_loss: 0.4798 - age_output_loss: 1.3072 - weight_output_loss: 0.9193 - bag_output_loss: 0.8250 - footwear_output_loss: 0.4813 - pose_output_loss: 0.3484 - emotion_output_loss: 0.8410 - gender_output_acc: 0.9465 - image_quality_output_acc: 0.7899 - age_output_acc: 0.4493 - weight_output_acc: 0.6673 - bag_output_acc: 0.6347 - footwear_output_acc: 0.7896 - pose_output_acc: 0.8632 - emotion_output_acc: 0.7141 - val_loss: 10.4264 - val_gender_output_loss: 0.8033 - val_image_quality_output_loss: 1.5705 - val_age_output_loss: 1.4137 - val_weight_output_loss: 0.9749 - val_bag_output_loss: 0.8946 - val_footwear_output_loss: 1.4612 - val_pose_output_loss: 1.5798 - val_emotion_output_loss: 0.9349 - val_gender_output_acc: 0.7278 - val_image_quality_output_acc: 0.5469 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5650 - val_footwear_output_acc: 0.6074 - val_pose_output_acc: 0.4415 - val_emotion_output_acc: 0.7087\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.6292 - gender_output_loss: 0.0855 - image_quality_output_loss: 0.2845 - age_output_loss: 1.2702 - weight_output_loss: 0.8929 - bag_output_loss: 0.8113 - footwear_output_loss: 0.4376 - pose_output_loss: 0.2292 - emotion_output_loss: 0.8268 - gender_output_acc: 0.9689 - image_quality_output_acc: 0.8793 - age_output_acc: 0.4633 - weight_output_acc: 0.6812 - bag_output_acc: 0.6453 - footwear_output_acc: 0.8048 - pose_output_acc: 0.9126 - emotion_output_acc: 0.7154 - val_loss: 9.9834 - val_gender_output_loss: 0.7244 - val_image_quality_output_loss: 1.9814 - val_age_output_loss: 1.4449 - val_weight_output_loss: 0.9621 - val_bag_output_loss: 0.8728 - val_footwear_output_loss: 1.1986 - val_pose_output_loss: 1.1176 - val_emotion_output_loss: 0.8910 - val_gender_output_acc: 0.8090 - val_image_quality_output_acc: 0.5645 - val_age_output_acc: 0.3866 - val_weight_output_acc: 0.6482 - val_bag_output_acc: 0.5882 - val_footwear_output_acc: 0.6557 - val_pose_output_acc: 0.6190 - val_emotion_output_acc: 0.7087\n",
            "Epoch 24/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.5440 - gender_output_loss: 0.0722 - image_quality_output_loss: 0.2597 - age_output_loss: 1.2616 - weight_output_loss: 0.8888 - bag_output_loss: 0.8141 - footwear_output_loss: 0.4322 - pose_output_loss: 0.2060 - emotion_output_loss: 0.8197 - gender_output_acc: 0.9732 - image_quality_output_acc: 0.8909 - age_output_acc: 0.4689 - weight_output_acc: 0.6816 - bag_output_acc: 0.6410 - footwear_output_acc: 0.8086 - pose_output_acc: 0.9232 - emotion_output_acc: 0.7169 - val_loss: 10.2884 - val_gender_output_loss: 0.8829 - val_image_quality_output_loss: 1.8664 - val_age_output_loss: 1.4678 - val_weight_output_loss: 0.9705 - val_bag_output_loss: 0.8823 - val_footwear_output_loss: 1.3284 - val_pose_output_loss: 1.2132 - val_emotion_output_loss: 0.8888 - val_gender_output_acc: 0.8049 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.3841 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5837 - val_footwear_output_acc: 0.6482 - val_pose_output_acc: 0.6326 - val_emotion_output_acc: 0.7087\n",
            "Epoch 25/50\n",
            "359/360 [============================>.] - ETA: 0s - loss: 5.8287 - gender_output_loss: 0.1022 - image_quality_output_loss: 0.3653 - age_output_loss: 1.2807 - weight_output_loss: 0.9033 - bag_output_loss: 0.8210 - footwear_output_loss: 0.4611 - pose_output_loss: 0.2834 - emotion_output_loss: 0.8244 - gender_output_acc: 0.9627 - image_quality_output_acc: 0.8407 - age_output_acc: 0.4588 - weight_output_acc: 0.6739 - bag_output_acc: 0.6368 - footwear_output_acc: 0.7983 - pose_output_acc: 0.8947 - emotion_output_acc: 0.7140\n",
            "360/360 [==============================] - 47s 131ms/step - loss: 5.8316 - gender_output_loss: 0.1027 - image_quality_output_loss: 0.3662 - age_output_loss: 1.2812 - weight_output_loss: 0.9038 - bag_output_loss: 0.8210 - footwear_output_loss: 0.4612 - pose_output_loss: 0.2839 - emotion_output_loss: 0.8243 - gender_output_acc: 0.9625 - image_quality_output_acc: 0.8405 - age_output_acc: 0.4584 - weight_output_acc: 0.6735 - bag_output_acc: 0.6366 - footwear_output_acc: 0.7984 - pose_output_acc: 0.8944 - emotion_output_acc: 0.7141 - val_loss: 10.8229 - val_gender_output_loss: 1.6793 - val_image_quality_output_loss: 1.5024 - val_age_output_loss: 1.4511 - val_weight_output_loss: 0.9830 - val_bag_output_loss: 0.9195 - val_footwear_output_loss: 1.6049 - val_pose_output_loss: 0.9910 - val_emotion_output_loss: 0.9012 - val_gender_output_acc: 0.6860 - val_image_quality_output_acc: 0.5186 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5433 - val_footwear_output_acc: 0.5862 - val_pose_output_acc: 0.6583 - val_emotion_output_acc: 0.7056\n",
            "Epoch 26/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 6.0310 - gender_output_loss: 0.1272 - image_quality_output_loss: 0.4325 - age_output_loss: 1.2898 - weight_output_loss: 0.9140 - bag_output_loss: 0.8263 - footwear_output_loss: 0.4863 - pose_output_loss: 0.3294 - emotion_output_loss: 0.8328 - gender_output_acc: 0.9539 - image_quality_output_acc: 0.8090 - age_output_acc: 0.4533 - weight_output_acc: 0.6693 - bag_output_acc: 0.6353 - footwear_output_acc: 0.7868 - pose_output_acc: 0.8721 - emotion_output_acc: 0.7146 - val_loss: 10.3852 - val_gender_output_loss: 0.7584 - val_image_quality_output_loss: 1.8047 - val_age_output_loss: 1.6086 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.9063 - val_footwear_output_loss: 1.2263 - val_pose_output_loss: 1.4175 - val_emotion_output_loss: 0.8947 - val_gender_output_acc: 0.7999 - val_image_quality_output_acc: 0.5680 - val_age_output_acc: 0.3579 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5932 - val_footwear_output_acc: 0.6562 - val_pose_output_acc: 0.4919 - val_emotion_output_acc: 0.7087\n",
            "Epoch 26/50\n",
            "Epoch 27/50\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 5.6421 - gender_output_loss: 0.0901 - image_quality_output_loss: 0.3050 - age_output_loss: 1.2546 - weight_output_loss: 0.8954 - bag_output_loss: 0.8153 - footwear_output_loss: 0.4380 - pose_output_loss: 0.2307 - emotion_output_loss: 0.8216 - gender_output_acc: 0.9674 - image_quality_output_acc: 0.8743 - age_output_acc: 0.4720 - weight_output_acc: 0.6764 - bag_output_acc: 0.6424 - footwear_output_acc: 0.8080 - pose_output_acc: 0.9167 - emotion_output_acc: 0.7176 - val_loss: 9.9850 - val_gender_output_loss: 0.6903 - val_image_quality_output_loss: 2.0170 - val_age_output_loss: 1.4433 - val_weight_output_loss: 0.9638 - val_bag_output_loss: 0.8778 - val_footwear_output_loss: 1.1461 - val_pose_output_loss: 1.1591 - val_emotion_output_loss: 0.8976 - val_gender_output_acc: 0.8054 - val_image_quality_output_acc: 0.5721 - val_age_output_acc: 0.3846 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.5912 - val_footwear_output_acc: 0.6618 - val_pose_output_acc: 0.6048 - val_emotion_output_acc: 0.7087\n",
            "Epoch 28/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.4581 - gender_output_loss: 0.0706 - image_quality_output_loss: 0.2361 - age_output_loss: 1.2367 - weight_output_loss: 0.8830 - bag_output_loss: 0.8152 - footwear_output_loss: 0.4264 - pose_output_loss: 0.1863 - emotion_output_loss: 0.8141 - gender_output_acc: 0.9750 - image_quality_output_acc: 0.9058 - age_output_acc: 0.4791 - weight_output_acc: 0.6867 - bag_output_acc: 0.6421 - footwear_output_acc: 0.8122 - pose_output_acc: 0.9343 - emotion_output_acc: 0.7189 - val_loss: 10.8035 - val_gender_output_loss: 0.9727 - val_image_quality_output_loss: 2.4255 - val_age_output_loss: 1.4502 - val_weight_output_loss: 0.9647 - val_bag_output_loss: 0.8695 - val_footwear_output_loss: 1.1728 - val_pose_output_loss: 1.2511 - val_emotion_output_loss: 0.9085 - val_gender_output_acc: 0.7707 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3926 - val_weight_output_acc: 0.6436 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6517 - val_pose_output_acc: 0.5464 - val_emotion_output_acc: 0.7087\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.5609 - gender_output_loss: 0.0812 - image_quality_output_loss: 0.2748 - age_output_loss: 1.2400 - weight_output_loss: 0.8877 - bag_output_loss: 0.8165 - footwear_output_loss: 0.4375 - pose_output_loss: 0.2165 - emotion_output_loss: 0.8198 - gender_output_acc: 0.9686 - image_quality_output_acc: 0.8842 - age_output_acc: 0.4780 - weight_output_acc: 0.6830 - bag_output_acc: 0.6444 - footwear_output_acc: 0.8082 - pose_output_acc: 0.9193 - emotion_output_acc: 0.7194 - val_loss: 11.2068 - val_gender_output_loss: 0.8025 - val_image_quality_output_loss: 2.1610 - val_age_output_loss: 1.4460 - val_weight_output_loss: 0.9780 - val_bag_output_loss: 0.8856 - val_footwear_output_loss: 1.2427 - val_pose_output_loss: 1.9726 - val_emotion_output_loss: 0.9323 - val_gender_output_acc: 0.7853 - val_image_quality_output_acc: 0.5590 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6442 - val_bag_output_acc: 0.5811 - val_footwear_output_acc: 0.6487 - val_pose_output_acc: 0.4682 - val_emotion_output_acc: 0.7092\n",
            "Epoch 30/50\n",
            "Epoch 29/50\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 6.0157 - gender_output_loss: 0.1277 - image_quality_output_loss: 0.4349 - age_output_loss: 1.2813 - weight_output_loss: 0.9122 - bag_output_loss: 0.8278 - footwear_output_loss: 0.4883 - pose_output_loss: 0.3234 - emotion_output_loss: 0.8291 - gender_output_acc: 0.9516 - image_quality_output_acc: 0.8076 - age_output_acc: 0.4577 - weight_output_acc: 0.6684 - bag_output_acc: 0.6330 - footwear_output_acc: 0.7891 - pose_output_acc: 0.8766 - emotion_output_acc: 0.7155 - val_loss: 10.4045 - val_gender_output_loss: 1.0581 - val_image_quality_output_loss: 2.0043 - val_age_output_loss: 1.4268 - val_weight_output_loss: 0.9701 - val_bag_output_loss: 0.9108 - val_footwear_output_loss: 1.0371 - val_pose_output_loss: 1.2843 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.7162 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.6421 - val_pose_output_acc: 0.4819 - val_emotion_output_acc: 0.7087\n",
            "Epoch 31/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.7123 - gender_output_loss: 0.1007 - image_quality_output_loss: 0.3286 - age_output_loss: 1.2576 - weight_output_loss: 0.8961 - bag_output_loss: 0.8211 - footwear_output_loss: 0.4448 - pose_output_loss: 0.2461 - emotion_output_loss: 0.8238 - gender_output_acc: 0.9635 - image_quality_output_acc: 0.8580 - age_output_acc: 0.4712 - weight_output_acc: 0.6775 - bag_output_acc: 0.6358 - footwear_output_acc: 0.8024 - pose_output_acc: 0.9051 - emotion_output_acc: 0.7188 - val_loss: 10.2495 - val_gender_output_loss: 0.6898 - val_image_quality_output_loss: 1.9254 - val_age_output_loss: 1.5485 - val_weight_output_loss: 0.9663 - val_bag_output_loss: 0.8751 - val_footwear_output_loss: 1.2001 - val_pose_output_loss: 1.3610 - val_emotion_output_loss: 0.8912 - val_gender_output_acc: 0.8185 - val_image_quality_output_acc: 0.5650 - val_age_output_acc: 0.3710 - val_weight_output_acc: 0.6381 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6552 - val_pose_output_acc: 0.5217 - val_emotion_output_acc: 0.7082\n",
            "Epoch 32/50\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 6.0157 - gender_output_loss: 0.1277 - image_quality_output_loss: 0.4349 - age_output_loss: 1.2813 - weight_output_loss: 0.9122 - bag_output_loss: 0.8278 - footwear_output_loss: 0.4883 - pose_output_loss: 0.3234 - emotion_output_loss: 0.8291 - gender_output_acc: 0.9516 - image_quality_output_acc: 0.8076 - age_output_acc: 0.4577 - weight_output_acc: 0.6684 - bag_output_acc: 0.6330 - footwear_output_acc: 0.7891 - pose_output_acc: 0.8766 - emotion_output_acc: 0.7155 - val_loss: 10.4045 - val_gender_output_loss: 1.0581 - val_image_quality_output_loss: 2.0043 - val_age_output_loss: 1.4268 - val_weight_output_loss: 0.9701 - val_bag_output_loss: 0.9108 - val_footwear_output_loss: 1.0371 - val_pose_output_loss: 1.2843 - val_emotion_output_loss: 0.9187 - val_gender_output_acc: 0.7162 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6431 - val_bag_output_acc: 0.5585 - val_footwear_output_acc: 0.6421 - val_pose_output_acc: 0.4819 - val_emotion_output_acc: 0.7087\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 5.4483 - gender_output_loss: 0.0776 - image_quality_output_loss: 0.2465 - age_output_loss: 1.2222 - weight_output_loss: 0.8746 - bag_output_loss: 0.8133 - footwear_output_loss: 0.4260 - pose_output_loss: 0.1863 - emotion_output_loss: 0.8103 - gender_output_acc: 0.9725 - image_quality_output_acc: 0.8985 - age_output_acc: 0.4850 - weight_output_acc: 0.6874 - bag_output_acc: 0.6431 - footwear_output_acc: 0.8120 - pose_output_acc: 0.9336 - emotion_output_acc: 0.7218 - val_loss: 10.5916 - val_gender_output_loss: 0.7141 - val_image_quality_output_loss: 2.0919 - val_age_output_loss: 1.6104 - val_weight_output_loss: 0.9675 - val_bag_output_loss: 0.8840 - val_footwear_output_loss: 1.2574 - val_pose_output_loss: 1.3775 - val_emotion_output_loss: 0.8980 - val_gender_output_acc: 0.8135 - val_image_quality_output_acc: 0.5635 - val_age_output_acc: 0.3579 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6537 - val_pose_output_acc: 0.5146 - val_emotion_output_acc: 0.7072\n",
            "Epoch 33/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.4311 - gender_output_loss: 0.0703 - image_quality_output_loss: 0.2415 - age_output_loss: 1.2145 - weight_output_loss: 0.8738 - bag_output_loss: 0.8163 - footwear_output_loss: 0.4290 - pose_output_loss: 0.1914 - emotion_output_loss: 0.8052 - gender_output_acc: 0.9741 - image_quality_output_acc: 0.9003 - age_output_acc: 0.4885 - weight_output_acc: 0.6872 - bag_output_acc: 0.6428 - footwear_output_acc: 0.8135 - pose_output_acc: 0.9312 - emotion_output_acc: 0.7229 - val_loss: 10.6946 - val_gender_output_loss: 0.8447 - val_image_quality_output_loss: 2.3818 - val_age_output_loss: 1.4725 - val_weight_output_loss: 0.9767 - val_bag_output_loss: 0.8780 - val_footwear_output_loss: 1.1421 - val_pose_output_loss: 1.2992 - val_emotion_output_loss: 0.9122 - val_gender_output_acc: 0.8004 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3957 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6492 - val_pose_output_acc: 0.5963 - val_emotion_output_acc: 0.7021\n",
            "Epoch 34/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.8905 - gender_output_loss: 0.1275 - image_quality_output_loss: 0.4120 - age_output_loss: 1.2653 - weight_output_loss: 0.8989 - bag_output_loss: 0.8242 - footwear_output_loss: 0.4714 - pose_output_loss: 0.2808 - emotion_output_loss: 0.8212 - gender_output_acc: 0.9529 - image_quality_output_acc: 0.8218 - age_output_acc: 0.4633 - weight_output_acc: 0.6740 - bag_output_acc: 0.6415 - footwear_output_acc: 0.7928 - pose_output_acc: 0.8949 - emotion_output_acc: 0.7188 - val_loss: 10.1276 - val_gender_output_loss: 0.6230 - val_image_quality_output_loss: 2.0385 - val_age_output_loss: 1.4623 - val_weight_output_loss: 0.9753 - val_bag_output_loss: 0.8827 - val_footwear_output_loss: 1.3025 - val_pose_output_loss: 1.1397 - val_emotion_output_loss: 0.9090 - val_gender_output_acc: 0.7828 - val_image_quality_output_acc: 0.5428 - val_age_output_acc: 0.3952 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5877 - val_footwear_output_acc: 0.6220 - val_pose_output_acc: 0.4662 - val_emotion_output_acc: 0.7061\n",
            "Epoch 35/50\n",
            "360/360 [==============================] - 47s 129ms/step - loss: 5.7909 - gender_output_loss: 0.1108 - image_quality_output_loss: 0.3528 - age_output_loss: 1.2490 - weight_output_loss: 0.8993 - bag_output_loss: 0.8284 - footwear_output_loss: 0.4698 - pose_output_loss: 0.2652 - emotion_output_loss: 0.8206 - gender_output_acc: 0.9571 - image_quality_output_acc: 0.8549 - age_output_acc: 0.4734 - weight_output_acc: 0.6741 - bag_output_acc: 0.6379 - footwear_output_acc: 0.7977 - pose_output_acc: 0.9020 - emotion_output_acc: 0.7188 - val_loss: 10.4250 - val_gender_output_loss: 0.7704 - val_image_quality_output_loss: 1.8910 - val_age_output_loss: 1.4814 - val_weight_output_loss: 0.9659 - val_bag_output_loss: 0.8833 - val_footwear_output_loss: 1.0930 - val_pose_output_loss: 1.6314 - val_emotion_output_loss: 0.9145 - val_gender_output_acc: 0.8014 - val_image_quality_output_acc: 0.5670 - val_age_output_acc: 0.3896 - val_weight_output_acc: 0.6421 - val_bag_output_acc: 0.5867 - val_footwear_output_acc: 0.6537 - val_pose_output_acc: 0.4561 - val_emotion_output_acc: 0.7087\n",
            "Epoch 36/50\n",
            "360/360 [==============================] - 46s 126ms/step - loss: 5.4535 - gender_output_loss: 0.0811 - image_quality_output_loss: 0.2529 - age_output_loss: 1.2094 - weight_output_loss: 0.8766 - bag_output_loss: 0.8171 - footwear_output_loss: 0.4308 - pose_output_loss: 0.1872 - emotion_output_loss: 0.8056 - gender_output_acc: 0.9702 - image_quality_output_acc: 0.8977 - age_output_acc: 0.4919 - weight_output_acc: 0.6862 - bag_output_acc: 0.6465 - footwear_output_acc: 0.8118 - pose_output_acc: 0.9324 - emotion_output_acc: 0.7231 - val_loss: 10.3289 - val_gender_output_loss: 0.7447 - val_image_quality_output_loss: 2.0675 - val_age_output_loss: 1.4817 - val_weight_output_loss: 0.9662 - val_bag_output_loss: 0.8748 - val_footwear_output_loss: 1.1843 - val_pose_output_loss: 1.3073 - val_emotion_output_loss: 0.9102 - val_gender_output_acc: 0.7989 - val_image_quality_output_acc: 0.5620 - val_age_output_acc: 0.3886 - val_weight_output_acc: 0.6416 - val_bag_output_acc: 0.5927 - val_footwear_output_acc: 0.6547 - val_pose_output_acc: 0.5570 - val_emotion_output_acc: 0.7077\n",
            "Epoch 37/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.3465 - gender_output_loss: 0.0668 - image_quality_output_loss: 0.2214 - age_output_loss: 1.1898 - weight_output_loss: 0.8644 - bag_output_loss: 0.8166 - footwear_output_loss: 0.4211 - pose_output_loss: 0.1722 - emotion_output_loss: 0.8030 - gender_output_acc: 0.9765 - image_quality_output_acc: 0.9115 - age_output_acc: 0.4987 - weight_output_acc: 0.6898 - bag_output_acc: 0.6470 - footwear_output_acc: 0.8169 - pose_output_acc: 0.9365 - emotion_output_acc: 0.7253 - val_loss: 11.0441 - val_gender_output_loss: 0.8321 - val_image_quality_output_loss: 2.0756 - val_age_output_loss: 1.4455 - val_weight_output_loss: 0.9666 - val_bag_output_loss: 0.8759 - val_footwear_output_loss: 1.3798 - val_pose_output_loss: 1.7709 - val_emotion_output_loss: 0.9081 - val_gender_output_acc: 0.7928 - val_image_quality_output_acc: 0.5539 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6447 - val_bag_output_acc: 0.5907 - val_footwear_output_acc: 0.6341 - val_pose_output_acc: 0.5252 - val_emotion_output_acc: 0.7072\n",
            "Epoch 38/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.5517 - gender_output_loss: 0.0913 - image_quality_output_loss: 0.2940 - age_output_loss: 1.2104 - weight_output_loss: 0.8795 - bag_output_loss: 0.8181 - footwear_output_loss: 0.4393 - pose_output_loss: 0.2237 - emotion_output_loss: 0.8069 - gender_output_acc: 0.9680 - image_quality_output_acc: 0.8753 - age_output_acc: 0.4872 - weight_output_acc: 0.6832 - bag_output_acc: 0.6434 - footwear_output_acc: 0.8100 - pose_output_acc: 0.9184 - emotion_output_acc: 0.7240 - val_loss: 14.2694 - val_gender_output_loss: 1.2457 - val_image_quality_output_loss: 1.9067 - val_age_output_loss: 2.6340 - val_weight_output_loss: 1.2891 - val_bag_output_loss: 0.9763 - val_footwear_output_loss: 2.2707 - val_pose_output_loss: 2.2382 - val_emotion_output_loss: 0.9182 - val_gender_output_acc: 0.7611 - val_image_quality_output_acc: 0.5307 - val_age_output_acc: 0.2949 - val_weight_output_acc: 0.5025 - val_bag_output_acc: 0.5827 - val_footwear_output_acc: 0.5439 - val_pose_output_acc: 0.4052 - val_emotion_output_acc: 0.7087\n",
            "Epoch 39/50\n",
            "360/360 [==============================] - 46s 126ms/step - loss: 5.8589 - gender_output_loss: 0.1182 - image_quality_output_loss: 0.3884 - age_output_loss: 1.2496 - weight_output_loss: 0.8935 - bag_output_loss: 0.8264 - footwear_output_loss: 0.4874 - pose_output_loss: 0.2865 - emotion_output_loss: 0.8144 - gender_output_acc: 0.9543 - image_quality_output_acc: 0.8329 - age_output_acc: 0.4741 - weight_output_acc: 0.6739 - bag_output_acc: 0.6403 - footwear_output_acc: 0.7922 - pose_output_acc: 0.8927 - emotion_output_acc: 0.7212 - val_loss: 10.7215 - val_gender_output_loss: 0.6727 - val_image_quality_output_loss: 2.0879 - val_age_output_loss: 1.5366 - val_weight_output_loss: 0.9721 - val_bag_output_loss: 0.8946 - val_footwear_output_loss: 1.7055 - val_pose_output_loss: 1.1407 - val_emotion_output_loss: 0.9157 - val_gender_output_acc: 0.8054 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.3911 - val_weight_output_acc: 0.6366 - val_bag_output_acc: 0.5993 - val_footwear_output_acc: 0.6038 - val_pose_output_acc: 0.5731 - val_emotion_output_acc: 0.6941\n",
            "Epoch 40/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.4758 - gender_output_loss: 0.0808 - image_quality_output_loss: 0.2631 - age_output_loss: 1.2003 - weight_output_loss: 0.8730 - bag_output_loss: 0.8214 - footwear_output_loss: 0.4371 - pose_output_loss: 0.2031 - emotion_output_loss: 0.8028 - gender_output_acc: 0.9713 - image_quality_output_acc: 0.8936 - age_output_acc: 0.4921 - weight_output_acc: 0.6847 - bag_output_acc: 0.6464 - footwear_output_acc: 0.8102 - pose_output_acc: 0.9256 - emotion_output_acc: 0.7262 - val_loss: 10.5350 - val_gender_output_loss: 0.9402 - val_image_quality_output_loss: 2.1067 - val_age_output_loss: 1.5308 - val_weight_output_loss: 0.9650 - val_bag_output_loss: 0.8721 - val_footwear_output_loss: 1.1595 - val_pose_output_loss: 1.2448 - val_emotion_output_loss: 0.9231 - val_gender_output_acc: 0.7656 - val_image_quality_output_acc: 0.5600 - val_age_output_acc: 0.3705 - val_weight_output_acc: 0.6467 - val_bag_output_acc: 0.5943 - val_footwear_output_acc: 0.6487 - val_pose_output_acc: 0.5650 - val_emotion_output_acc: 0.7051\n",
            "Epoch 41/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.2968 - gender_output_loss: 0.0664 - image_quality_output_loss: 0.2220 - age_output_loss: 1.1659 - weight_output_loss: 0.8583 - bag_output_loss: 0.8157 - footwear_output_loss: 0.4140 - pose_output_loss: 0.1690 - emotion_output_loss: 0.7932 - gender_output_acc: 0.9758 - image_quality_output_acc: 0.9115 - age_output_acc: 0.5055 - weight_output_acc: 0.6910 - bag_output_acc: 0.6518 - footwear_output_acc: 0.8218 - pose_output_acc: 0.9399 - emotion_output_acc: 0.7305 - val_loss: 10.9019 - val_gender_output_loss: 0.8301 - val_image_quality_output_loss: 2.4431 - val_age_output_loss: 1.4586 - val_weight_output_loss: 0.9709 - val_bag_output_loss: 0.8713 - val_footwear_output_loss: 1.1205 - val_pose_output_loss: 1.4868 - val_emotion_output_loss: 0.9293 - val_gender_output_acc: 0.7833 - val_image_quality_output_acc: 0.5580 - val_age_output_acc: 0.3821 - val_weight_output_acc: 0.6472 - val_bag_output_acc: 0.6008 - val_footwear_output_acc: 0.6608 - val_pose_output_acc: 0.5000 - val_emotion_output_acc: 0.7067\n",
            "Epoch 42/50\n",
            "360/360 [==============================] - 46s 126ms/step - loss: 5.3886 - gender_output_loss: 0.0791 - image_quality_output_loss: 0.2561 - age_output_loss: 1.1761 - weight_output_loss: 0.8662 - bag_output_loss: 0.8224 - footwear_output_loss: 0.4246 - pose_output_loss: 0.1811 - emotion_output_loss: 0.7932 - gender_output_acc: 0.9720 - image_quality_output_acc: 0.8948 - age_output_acc: 0.5000 - weight_output_acc: 0.6878 - bag_output_acc: 0.6499 - footwear_output_acc: 0.8172 - pose_output_acc: 0.9347 - emotion_output_acc: 0.7309 - val_loss: 11.9226 - val_gender_output_loss: 0.9625 - val_image_quality_output_loss: 1.9885 - val_age_output_loss: 1.6902 - val_weight_output_loss: 0.9881 - val_bag_output_loss: 0.8911 - val_footwear_output_loss: 1.4136 - val_pose_output_loss: 2.2579 - val_emotion_output_loss: 0.9419 - val_gender_output_acc: 0.7792 - val_image_quality_output_acc: 0.5464 - val_age_output_acc: 0.3760 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6326 - val_pose_output_acc: 0.5570 - val_emotion_output_acc: 0.7082\n",
            "Epoch 43/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.8136 - gender_output_loss: 0.1133 - image_quality_output_loss: 0.3799 - age_output_loss: 1.2290 - weight_output_loss: 0.8974 - bag_output_loss: 0.8262 - footwear_output_loss: 0.4722 - pose_output_loss: 0.2881 - emotion_output_loss: 0.8149 - gender_output_acc: 0.9584 - image_quality_output_acc: 0.8365 - age_output_acc: 0.4805 - weight_output_acc: 0.6704 - bag_output_acc: 0.6407 - footwear_output_acc: 0.8009 - pose_output_acc: 0.8911 - emotion_output_acc: 0.7194 - val_loss: 10.6964 - val_gender_output_loss: 1.5468 - val_image_quality_output_loss: 1.7933 - val_age_output_loss: 1.5916 - val_weight_output_loss: 0.9821 - val_bag_output_loss: 0.8904 - val_footwear_output_loss: 1.0952 - val_pose_output_loss: 1.0967 - val_emotion_output_loss: 0.9036 - val_gender_output_acc: 0.6794 - val_image_quality_output_acc: 0.5454 - val_age_output_acc: 0.3821 - val_weight_output_acc: 0.6426 - val_bag_output_acc: 0.5741 - val_footwear_output_acc: 0.6245 - val_pose_output_acc: 0.6724 - val_emotion_output_acc: 0.7056\n",
            "Epoch 44/50\n",
            "360/360 [==============================] - 46s 129ms/step - loss: 5.5281 - gender_output_loss: 0.0890 - image_quality_output_loss: 0.2951 - age_output_loss: 1.1940 - weight_output_loss: 0.8763 - bag_output_loss: 0.8228 - footwear_output_loss: 0.4452 - pose_output_loss: 0.2080 - emotion_output_loss: 0.8020 - gender_output_acc: 0.9661 - image_quality_output_acc: 0.8753 - age_output_acc: 0.4905 - weight_output_acc: 0.6808 - bag_output_acc: 0.6453 - footwear_output_acc: 0.8066 - pose_output_acc: 0.9244 - emotion_output_acc: 0.7260 - val_loss: 10.7563 - val_gender_output_loss: 0.8959 - val_image_quality_output_loss: 1.9468 - val_age_output_loss: 1.7625 - val_weight_output_loss: 0.9675 - val_bag_output_loss: 0.8772 - val_footwear_output_loss: 1.3916 - val_pose_output_loss: 1.2121 - val_emotion_output_loss: 0.9083 - val_gender_output_acc: 0.8049 - val_image_quality_output_acc: 0.5565 - val_age_output_acc: 0.3604 - val_weight_output_acc: 0.6406 - val_bag_output_acc: 0.5968 - val_footwear_output_acc: 0.6371 - val_pose_output_acc: 0.5983 - val_emotion_output_acc: 0.7072\n",
            "Epoch 45/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.2685 - gender_output_loss: 0.0744 - image_quality_output_loss: 0.2280 - age_output_loss: 1.1421 - weight_output_loss: 0.8512 - bag_output_loss: 0.8154 - footwear_output_loss: 0.4157 - pose_output_loss: 0.1629 - emotion_output_loss: 0.7854 - gender_output_acc: 0.9722 - image_quality_output_acc: 0.9086 - age_output_acc: 0.5124 - weight_output_acc: 0.6952 - bag_output_acc: 0.6519 - footwear_output_acc: 0.8213 - pose_output_acc: 0.9430 - emotion_output_acc: 0.7294 - val_loss: 10.7155 - val_gender_output_loss: 0.8760 - val_image_quality_output_loss: 2.3117 - val_age_output_loss: 1.6236 - val_weight_output_loss: 0.9711 - val_bag_output_loss: 0.8712 - val_footwear_output_loss: 1.0786 - val_pose_output_loss: 1.2686 - val_emotion_output_loss: 0.9220 - val_gender_output_acc: 0.7944 - val_image_quality_output_acc: 0.5670 - val_age_output_acc: 0.3664 - val_weight_output_acc: 0.6452 - val_bag_output_acc: 0.5983 - val_footwear_output_acc: 0.6628 - val_pose_output_acc: 0.5640 - val_emotion_output_acc: 0.7087\n",
            "Epoch 46/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.2224 - gender_output_loss: 0.0678 - image_quality_output_loss: 0.2251 - age_output_loss: 1.1342 - weight_output_loss: 0.8453 - bag_output_loss: 0.8132 - footwear_output_loss: 0.4115 - pose_output_loss: 0.1491 - emotion_output_loss: 0.7848 - gender_output_acc: 0.9753 - image_quality_output_acc: 0.9051 - age_output_acc: 0.5110 - weight_output_acc: 0.6949 - bag_output_acc: 0.6554 - footwear_output_acc: 0.8238 - pose_output_acc: 0.9464 - emotion_output_acc: 0.7325 - val_loss: 11.3680 - val_gender_output_loss: 0.9026 - val_image_quality_output_loss: 2.2246 - val_age_output_loss: 1.8305 - val_weight_output_loss: 0.9787 - val_bag_output_loss: 0.8818 - val_footwear_output_loss: 1.0540 - val_pose_output_loss: 1.7908 - val_emotion_output_loss: 0.9154 - val_gender_output_acc: 0.8105 - val_image_quality_output_acc: 0.5610 - val_age_output_acc: 0.3644 - val_weight_output_acc: 0.6396 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.4249 - val_emotion_output_acc: 0.7092\n",
            "Epoch 47/50\n",
            "360/360 [==============================] - 45s 126ms/step - loss: 5.6539 - gender_output_loss: 0.1080 - image_quality_output_loss: 0.3477 - age_output_loss: 1.1972 - weight_output_loss: 0.8776 - bag_output_loss: 0.8212 - footwear_output_loss: 0.4579 - pose_output_loss: 0.2520 - emotion_output_loss: 0.8015 - gender_output_acc: 0.9587 - image_quality_output_acc: 0.8529 - age_output_acc: 0.4898 - weight_output_acc: 0.6777 - bag_output_acc: 0.6492 - footwear_output_acc: 0.8062 - pose_output_acc: 0.9096 - emotion_output_acc: 0.7248 - val_loss: 10.4614 - val_gender_output_loss: 0.6854 - val_image_quality_output_loss: 2.2503 - val_age_output_loss: 1.6637 - val_weight_output_loss: 0.9793 - val_bag_output_loss: 0.8863 - val_footwear_output_loss: 1.1652 - val_pose_output_loss: 1.1302 - val_emotion_output_loss: 0.9047 - val_gender_output_acc: 0.7893 - val_image_quality_output_acc: 0.5403 - val_age_output_acc: 0.3690 - val_weight_output_acc: 0.6386 - val_bag_output_acc: 0.5902 - val_footwear_output_acc: 0.6225 - val_pose_output_acc: 0.5948 - val_emotion_output_acc: 0.7061\n",
            "Epoch 48/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.6464 - gender_output_loss: 0.1071 - image_quality_output_loss: 0.3383 - age_output_loss: 1.1921 - weight_output_loss: 0.8806 - bag_output_loss: 0.8237 - footwear_output_loss: 0.4588 - pose_output_loss: 0.2404 - emotion_output_loss: 0.8080 - gender_output_acc: 0.9619 - image_quality_output_acc: 0.8554 - age_output_acc: 0.4937 - weight_output_acc: 0.6786 - bag_output_acc: 0.6473 - footwear_output_acc: 0.8009 - pose_output_acc: 0.9120 - emotion_output_acc: 0.7234 - val_loss: 11.1667 - val_gender_output_loss: 0.7039 - val_image_quality_output_loss: 2.2344 - val_age_output_loss: 1.6330 - val_weight_output_loss: 0.9674 - val_bag_output_loss: 0.8796 - val_footwear_output_loss: 1.2990 - val_pose_output_loss: 1.7196 - val_emotion_output_loss: 0.9329 - val_gender_output_acc: 0.8039 - val_image_quality_output_acc: 0.5529 - val_age_output_acc: 0.3649 - val_weight_output_acc: 0.6482 - val_bag_output_acc: 0.5958 - val_footwear_output_acc: 0.6305 - val_pose_output_acc: 0.4138 - val_emotion_output_acc: 0.7031\n",
            "Epoch 49/50\n",
            "360/360 [==============================] - 46s 127ms/step - loss: 5.2920 - gender_output_loss: 0.0795 - image_quality_output_loss: 0.2473 - age_output_loss: 1.1319 - weight_output_loss: 0.8520 - bag_output_loss: 0.8157 - footwear_output_loss: 0.4177 - pose_output_loss: 0.1702 - emotion_output_loss: 0.7821 - gender_output_acc: 0.9713 - image_quality_output_acc: 0.8944 - age_output_acc: 0.5174 - weight_output_acc: 0.6902 - bag_output_acc: 0.6527 - footwear_output_acc: 0.8196 - pose_output_acc: 0.9406 - emotion_output_acc: 0.7308 - val_loss: 10.6241 - val_gender_output_loss: 0.7649 - val_image_quality_output_loss: 2.1357 - val_age_output_loss: 1.5657 - val_weight_output_loss: 0.9704 - val_bag_output_loss: 0.8789 - val_footwear_output_loss: 1.1963 - val_pose_output_loss: 1.3921 - val_emotion_output_loss: 0.9254 - val_gender_output_acc: 0.8009 - val_image_quality_output_acc: 0.5675 - val_age_output_acc: 0.3745 - val_weight_output_acc: 0.6462 - val_bag_output_acc: 0.5922 - val_footwear_output_acc: 0.6573 - val_pose_output_acc: 0.5413 - val_emotion_output_acc: 0.7077\n",
            "Epoch 50/50\n",
            "Epoch 49/50\n",
            "360/360 [==============================] - 46s 128ms/step - loss: 5.1737 - gender_output_loss: 0.0683 - image_quality_output_loss: 0.2173 - age_output_loss: 1.1077 - weight_output_loss: 0.8410 - bag_output_loss: 0.8091 - footwear_output_loss: 0.4091 - pose_output_loss: 0.1529 - emotion_output_loss: 0.7743 - gender_output_acc: 0.9750 - image_quality_output_acc: 0.9099 - age_output_acc: 0.5235 - weight_output_acc: 0.6975 - bag_output_acc: 0.6588 - footwear_output_acc: 0.8266 - pose_output_acc: 0.9474 - emotion_output_acc: 0.7334 - val_loss: 11.1397 - val_gender_output_loss: 0.8022 - val_image_quality_output_loss: 2.4364 - val_age_output_loss: 1.7970 - val_weight_output_loss: 0.9760 - val_bag_output_loss: 0.8805 - val_footwear_output_loss: 1.0931 - val_pose_output_loss: 1.4288 - val_emotion_output_loss: 0.9333 - val_gender_output_acc: 0.8059 - val_image_quality_output_acc: 0.5625 - val_age_output_acc: 0.3659 - val_weight_output_acc: 0.6401 - val_bag_output_acc: 0.5953 - val_footwear_output_acc: 0.6492 - val_pose_output_acc: 0.5076 - val_emotion_output_acc: 0.7087\n",
            "31/31 [==============================] - 4s 113ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age_output_acc': 0.3659274193548387,\n",
              " 'age_output_loss': 1.7970494416452223,\n",
              " 'bag_output_acc': 0.5952620967741935,\n",
              " 'bag_output_loss': 0.8804502775592189,\n",
              " 'emotion_output_acc': 0.7086693548387096,\n",
              " 'emotion_output_loss': 0.9332731769930932,\n",
              " 'footwear_output_acc': 0.6491935483870968,\n",
              " 'footwear_output_loss': 1.0930767963009496,\n",
              " 'gender_output_acc': 0.8059475806451613,\n",
              " 'gender_output_loss': 0.8021802200425056,\n",
              " 'image_quality_output_acc': 0.5625,\n",
              " 'image_quality_output_loss': 2.4364434596030944,\n",
              " 'loss': 11.139746173735588,\n",
              " 'pose_output_acc': 0.5075604838709677,\n",
              " 'pose_output_loss': 1.4288124153690953,\n",
              " 'weight_output_acc': 0.6401209677419355,\n",
              " 'weight_output_loss': 0.9759826717838165}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    }
  ]
}